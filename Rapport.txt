1.1 Execution d'un modele

	1.
		* map input records MIR = nombre de couples <cle,valeur> en entree de map.
		* map output records MOR = nombre de couples <cle,valeur> en sortie de map.
		
	2.
		* reduce input records RIR correspond au nombre de couples <cle,valeur> produit par map.
		RIR <= MOR.
			- RIR = MOR en l'absence de combiner entre un map reduce successif ou dans le cas ou il y a un combiner inneficace..
			- RIR < MOR dans le cas d'utilisation d'un combiner effectif entre 1 map reduce successif.
		
	3.
		* Apres un map, un traitement consistent a regrouper toutes les valeurs d'une meme cle est effectue.
		Le reduce qui suit le map utilise les couples <cle,liste(valeurs)> produit lors du traitement intermediaire.
		reduce input groups correspond au nombre de groupes ainsi calcule. 
	
1.2 Premier contact avec HDFS

	* Question:
		Chemin vers le reperetoire personnel = /user/<nom_user>
		Dans notre casm /user/kullae
		
1.3 Execution sur le cluster
	* Question:
		5 splits ou blocs sont lu sur HDFS, 1 pour chacun des 5 tomes des miserables.
		Le compteur "number of splits" indique cette valeur.
	
1.4 Combiner
	1.
		* On constate qu'il y a eu un certain nombre d'appels au combiner.
		Les compteurs "Combine input records" et "Combine output records" indiquent le nombre de
		couples <clé, valeur> en entrée et sortie du combiner.
		
	2.
		Dans le cas des 5 tomes des misérables, l'utilisation du combiner divise
		le nombre de données à traiter pour les reducers d'un facteur à peu prés égal à 4.
		On retrouve ce facteur sur la quantité de données:
			- écrite sur le disk en sortie de map (compteur "Map output materialized bytes"):
				--> 6750655 bytes sans combiner
				--> 1645046 bytes avec combiner

	3.
		* Il y a deux façons de connaitre le mot le plus utilisé:
			- en se servant du fait que hadoop renvoi un résultat trié par clé
			et donc en modifiant le reducer de wordcount. Au lieu définir la clé
			comme étant le mot, on choisit la somme.
			
			- avec la commande unix: sort -r -n -k 2 <fichier_resultat>
			
			Le premier couple <clé,valeur> indiqué en resultat et le mot le plus utilisé i.e
			"de" avec un total d'occurences de 16757.
			
1.5 Nombre de reducers
	* Question:
		Chaque reducer génére indépendemment un fichier résultat. Une fois que les mappers finissent
		de s'éxécuter, il n y a plus de raison de synchroniser les reducers qui étaient en attente du
		regroupement des clés. Chaque reducer peut gérer une ou un groupe de clé indépendemment des autres.
		Comme on a fixé le nombre de reducers à 3, on retrouve 3 fichiers résultats dans le répértoire.
		
		La politique de partitionnement Hadoop explique pourquoi il n y a qu'un seul reducer dans le cas
		ou ne spécifie pas explicitement le nombre de reducers à éxécuter.
		Par défaut, Hadoop en ajoute autant qu'il le juge utile.

1.6 In-Mapper Combiner

1.7 Compteur

2.1 Map et Reduce

2.2 Combiner
	
	Question:
		Dans le cas d'un combiner, il aurait fallut utiliser une structure de données contenant:
			- 1 champ pour le tag de type TEXT ou STRING
			- 1 champ pour le nombre d'occurence de type INTEGER
			
		Le map retournerait donc un couple clé valeur avec:
			- la clé étant l'identifiant du pays
			- la valeur étant la structure de données i.e les tags et leurs nombre d'occurences